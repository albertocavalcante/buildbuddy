"use strict";(self.webpackChunkbuildbuddy_docs_website=self.webpackChunkbuildbuddy_docs_website||[]).push([[2234],{4137:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return p}});var o=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=o.createContext({}),c=function(e){var t=o.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=c(e.components);return o.createElement(l.Provider,{value:t},e.children)},_={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},u=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=c(n),p=a,h=u["".concat(l,".").concat(p)]||u[p]||_[p]||i;return n?o.createElement(h,r(r({ref:t},d),{},{components:n})):o.createElement(h,r({ref:t},d))}));function p(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,r=new Array(i);r[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,r[1]=s;for(var c=2;c<i;c++)r[c]=n[c];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}u.displayName="MDXCreateElement"},4902:function(e,t,n){n.r(t),n.d(t,{contentTitle:function(){return b},default:function(){return k},frontMatter:function(){return g},metadata:function(){return m},toc:function(){return y}});var o=n(7462),a=n(3366),i=(n(7294),n(4137)),r=["components"],s={toc:[]};function l(e){var t=e.components,n=(0,a.Z)(e,r);return(0,i.kt)("wrapper",(0,o.Z)({},s,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'# Unstructured settings\n\n# app_directory (string): the directory containing app binary files to host\napp_directory: ""\n# auto_migrate_db (bool): If true, attempt to automigrate the db when \n# connecting\nauto_migrate_db: true\n# auto_migrate_db_and_exit (bool): If true, attempt to automigrate the db when \n# connecting, then exit the program.\nauto_migrate_db_and_exit: false\n# cache_stats_finalization_delay (time.Duration): The time allowed for all \n# metrics collectors across all apps to flush their local cache stats to the \n# backing storage, before finalizing stats in the DB.\ncache_stats_finalization_delay: 500ms\n# cleanup_interval (time.Duration): How often the janitor cleanup tasks will \n# run\ncleanup_interval: 10m0s\n# cleanup_workers (int): How many cleanup tasks to run\ncleanup_workers: 1\n# disable_ga (bool): If true; ga will be disabled\ndisable_ga: false\n# disable_telemetry (bool): If true; telemetry will be disabled\ndisable_telemetry: false\n# drop_invocation_pk_cols (bool): If true, attempt to drop invocation PK cols\ndrop_invocation_pk_cols: false\n# exit_when_ready (bool): If set, the app will exit as soon as it becomes \n# ready (useful for migrations)\nexit_when_ready: false\n# grpc_port (int): The port to listen for gRPC traffic on\ngrpc_port: 1985\n# grpcs_port (int): The port to listen for gRPCS traffic on\ngrpcs_port: 1986\n# internal_grpc_port (int): The port to listen for internal gRPC traffic on\ninternal_grpc_port: 1987\n# internal_grpcs_port (int): The port to listen for internal gRPCS traffic on\ninternal_grpcs_port: 1988\n# internal_http_port (int): The port to listen for internal HTTP traffic\ninternal_http_port: 0\n# js_entry_point_path (string): Absolute URL path of the app JS entry point\njs_entry_point_path: /app/app_bundle/app.js?hash={APP_BUNDLE_HASH}\n# listen (string): The interface to listen on (default: 0.0.0.0)\nlisten: 0.0.0.0\n# log_deletion_errors (bool): If true; log errors when ttl-deleting expired \n# data\nlog_deletion_errors: false\n# max_shutdown_duration (time.Duration): Time to wait for shutdown\nmax_shutdown_duration: 25s\n# migrate_disk_cache_to_v2_and_exit (bool): If true, attempt to migrate disk \n# cache to v2 layout.\nmigrate_disk_cache_to_v2_and_exit: false\n# monitoring_port (int): The port to listen for monitoring traffic on\nmonitoring_port: 9090\n# port (int): The port to listen for HTTP traffic on\nport: 8080\n# server_type (string): The server type to match on health checks\nserver_type: buildbuddy-server\n# shutdown_lameduck_duration (time.Duration): If set, the server will be \n# marked unready but not run shutdown functions until this period passes.\nshutdown_lameduck_duration: 0s\n# ssl_port (int): The port to listen for HTTPS traffic on\nssl_port: 8081\n# static_directory (string): the directory containing static files to host\nstatic_directory: ""\n# telemetry_endpoint (string): The telemetry endpoint to use\ntelemetry_endpoint: grpcs://t.buildbuddy.io:443\n# telemetry_interval (time.Duration): How often telemetry data will be \n# reported\ntelemetry_interval: 24h0m0s\n# verbose_telemetry_client (bool): If true; print telemetry client information\nverbose_telemetry_client: false\n\n# Structured settings\n\napi:\n    # api.api_key (string): The default API key to use for on-prem enterprise \n    # deploys with a single organization/group. **DEPRECATED** Manual API key \n    # specification is no longer supported; to retrieve specific API keys \n    # programmatically, please use the API key table. This field will still \n    # specify an API key to redact in case a manual API key was specified when \n    # buildbuddy was first set up.\n    api_key: ""\napp:\n    # app.admin_only_create_group (bool): If true, only admins of an existing \n    # group can create a new groups.\n    admin_only_create_group: false\n    # app.build_buddy_url (URL): The external URL where your BuildBuddy \n    # instance can be found.\n    build_buddy_url: http://localhost:8080\n    # app.cache_api_url (URL): Overrides the default remote cache protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    cache_api_url: ""\n    # app.code_editor_enabled (bool): If set, code editor functionality will \n    # be enabled.\n    code_editor_enabled: false\n    # app.default_to_dense_mode (bool): Enables the dense UI mode by default.\n    default_to_dense_mode: false\n    # app.disable_cert_config (bool): If true, the certificate based auth \n    # option will not be shown in the config widget.\n    disable_cert_config: false\n    # app.enable_grpc_metrics_by_group_id (bool): If enabled, grpc metrics by \n    # group ID will be recorded\n    enable_grpc_metrics_by_group_id: false\n    # app.enable_prometheus_histograms (bool): If true, collect prometheus \n    # histograms for all RPCs\n    enable_prometheus_histograms: true\n    # app.enable_read_target_statuses_from_olap_db (bool): If enabled, read \n    # target statuses from OLAP DB\n    enable_read_target_statuses_from_olap_db: false\n    # app.enable_structured_logging (bool): If true, log messages will be \n    # json-formatted.\n    enable_structured_logging: false\n    # app.enable_target_tracking (bool): Cloud-Only\n    enable_target_tracking: false\n    # app.enable_write_executions_to_olap_db (bool): If enabled, complete \n    # Executions will be flushed to OLAP DB\n    enable_write_executions_to_olap_db: false\n    # app.enable_write_test_target_statuses_to_olap_db (bool): If enabled, \n    # test target statuses will be flushed to OLAP DB\n    enable_write_test_target_statuses_to_olap_db: false\n    # app.enable_write_to_olap_db (bool): If enabled, complete invocations \n    # will be flushed to OLAP DB\n    enable_write_to_olap_db: false\n    # app.events_api_url (URL): Overrides the default build event protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    events_api_url: ""\n    # app.expanded_suggestions_enabled (bool): If set, enable more build \n    # suggestions in the UI.\n    expanded_suggestions_enabled: false\n    # app.global_filter_enabled (bool): If set, the global filter will be \n    # enabled in the UI.\n    global_filter_enabled: true\n    # app.grpc_max_recv_msg_size_bytes (int): Configures the max GRPC receive \n    # message size [bytes]\n    grpc_max_recv_msg_size_bytes: 50000000\n    # app.grpc_over_http_port_enabled (bool): Cloud-Only\n    grpc_over_http_port_enabled: false\n    # app.ignore_forced_tracing_header (bool): If set, we will not honor the \n    # forced tracing header.\n    ignore_forced_tracing_header: false\n    # app.log_enable_gcp_logging_format (bool): If true, the output structured \n    # logs will be compatible with format expected by GCP Logging.\n    log_enable_gcp_logging_format: false\n    # app.log_error_stack_traces (bool): If true, stack traces will be printed \n    # for errors that have them.\n    log_error_stack_traces: false\n    # app.log_gcp_log_id (string): The log ID to log to in GCP (if any).\n    log_gcp_log_id: ""\n    # app.log_gcp_project_id (string): The project ID to log to in GCP (if \n    # any).\n    log_gcp_project_id: ""\n    # app.log_include_short_file_name (bool): If true, log messages will \n    # include shortened originating file name.\n    log_include_short_file_name: false\n    # app.log_level (string): The desired log level. Logs with a level >= this \n    # level will be emitted. One of {\'fatal\', \'error\', \'warn\', \'info\', \n    # \'debug\'}\n    log_level: info\n    # app.remote_execution_api_url (URL): Overrides the default remote \n    # execution protocol gRPC address shown by BuildBuddy on the configuration \n    # screen.\n    remote_execution_api_url: ""\n    # app.test_grid_v2_enabled (bool): Whether to enable test grid V2\n    test_grid_v2_enabled: true\n    # app.test_output_manifests_enabled (bool): If set, the target page will \n    # render the contents of test output zips.\n    test_output_manifests_enabled: true\n    # app.trace_fraction (float64): Fraction of requests to sample for \n    # tracing.\n    trace_fraction: 0\n    # app.trace_fraction_overrides ([]string): Tracing fraction override based \n    # on name in format name=fraction.\n    trace_fraction_overrides: []\n    # app.trace_jaeger_collector (string): Address of the Jager collector \n    # endpoint where traces will be sent.\n    trace_jaeger_collector: ""\n    # app.trace_project_id (string): Optional GCP project ID to export traces \n    # to. If not specified, determined from default credentials or metadata \n    # server if running on GCP.\n    trace_project_id: ""\n    # app.trace_service_name (string): Name of the service to associate with \n    # traces.\n    trace_service_name: ""\n    # app.usage_enabled (bool): If set, the usage page will be enabled in the \n    # UI.\n    usage_enabled: false\n    # app.user_management_enabled (bool): If set, the user management page \n    # will be enabled in the UI.\n    user_management_enabled: true\n    # app.user_owned_keys_enabled (bool): If set, enable the UI controls for \n    # user-owned API keys.\n    user_owned_keys_enabled: false\nbuild_event_proxy:\n    # build_event_proxy.buffer_size (int): The number of build events to \n    # buffer locally when proxying build events.\n    buffer_size: 100\n    # build_event_proxy.hosts ([]string): The list of hosts to pass build \n    # events onto.\n    hosts: []\ncache:\n    client:\n        # cache.client.enable_upload_compression (bool): If true, enable \n        # compression of uploads to remote caches\n        enable_upload_compression: false\n    # cache.detailed_stats_enabled (bool): Whether to enable detailed stats \n    # recording for all cache requests.\n    detailed_stats_enabled: false\n    disk:\n        # cache.disk.partition_mappings ([]disk.PartitionMapping)\n        partition_mappings: []\n        # For example:\n        # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n\n        # cache.disk.partitions ([]disk.Partition)\n        partitions: []\n        # For example:\n        # - id: "" # The ID of the partition. (type: string)\n        #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n\n        # cache.disk.root_directory (string): The root directory to store all \n        # blobs in, if using disk based storage.\n        root_directory: ""\n        # cache.disk.use_v2_layout (bool): If enabled, files will be stored \n        # using the v2 layout. See disk_cache.MigrateToV2Layout for a \n        # description.\n        use_v2_layout: false\n    # cache.enable_tree_caching (bool): If true, cache GetTree responses (full \n    # and partial)\n    enable_tree_caching: true\n    # cache.in_memory (bool): Whether or not to use the in_memory cache.\n    in_memory: false\n    # cache.max_size_bytes (int64): How big to allow the cache to be (in \n    # bytes).\n    max_size_bytes: 10000000000\n    # cache.tree_cache_seed (string): If set, hash this with digests before \n    # caching / reading from tree cache\n    tree_cache_seed: treecache-07012022\n    # cache.zstd_transcoding_enabled (bool): Whether to accept requests to \n    # read/write zstd-compressed blobs, compressing/decompressing \n    # outgoing/incoming blobs on the fly.\n    zstd_transcoding_enabled: true\ndatabase:\n    # database.advanced_data_source (db.AdvancedConfig): Alternative to the \n    # database.data_source flag that allows finer control over database \n    # settings as well as allowing use of AWS IAM credentials. For most users, \n    # database.data_source is a simpler configuration method.\n    advanced_data_source:\n        driver: "" # The driver to use: one of sqlite3 or mysql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.advanced_read_replica (db.AdvancedConfig): Advanced alternative \n    # to database.read_replica. Refer to database.advanced for more \n    # information.\n    advanced_read_replica:\n        driver: "" # The driver to use: one of sqlite3 or mysql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.conn_max_lifetime_seconds (int): The maximum lifetime of a \n    # connection to the db\n    conn_max_lifetime_seconds: 0\n    # database.data_source (string): The SQL database to connect to, specified \n    # as a connection string.\n    data_source: sqlite3:///tmp/buildbuddy.db\n    # database.log_queries (bool): If true, log all queries\n    log_queries: false\n    # database.max_idle_conns (int): The maximum number of idle connections to \n    # maintain to the db\n    max_idle_conns: 0\n    # database.max_open_conns (int): The maximum number of open connections to \n    # maintain to the db\n    max_open_conns: 0\n    # database.read_replica (string): A secondary, read-only SQL database to \n    # connect to, specified as a connection string.\n    read_replica: ""\n    # database.slow_query_threshold (time.Duration): Queries longer than this \n    # duration will be logged with a \'Slow SQL\' warning.\n    slow_query_threshold: 500ms\n    # database.stats_poll_interval (time.Duration): How often to poll the DB \n    # client for connection stats (default: \'5s\').\n    stats_poll_interval: 5s\ngithub:\n    # github.access_token (string): The GitHub access token used to post \n    # GitHub commit statuses. ** Enterprise only **\n    access_token: ""\n    # github.client_id (string): The client ID of your GitHub Oauth App. ** \n    # Enterprise only **\n    client_id: ""\n    # github.client_secret (string): The client secret of your GitHub Oauth \n    # App. ** Enterprise only **\n    client_secret: ""\n    # github.status_name_suffix (string): Suffix to be appended to all \n    # reported GitHub status names. Useful for differentiating BuildBuddy \n    # deployments. For example: \'(dev)\' ** Enterprise only **\n    status_name_suffix: ""\n    # github.status_per_test_target (bool): If true, report status per test \n    # target. ** Enterprise only **\n    status_per_test_target: false\nintegrations:\n    invocation_upload:\n        # integrations.invocation_upload.enabled (bool): Whether to upload \n        # webhook data to the webhook URL configured per-Group. ** Enterprise \n        # only **\n        enabled: false\n        # integrations.invocation_upload.gcs_credentials (string): Credentials \n        # JSON for the Google service account used to authenticate when GCS is \n        # used as the invocation upload target. ** Enterprise only **\n        gcs_credentials: ""\n    slack:\n        # integrations.slack.webhook_url (string): A Slack webhook url to post \n        # build update messages to.\n        webhook_url: ""\nmonitoring:\n    basic_auth:\n        # monitoring.basic_auth.password (string): Optional password for basic \n        # auth on the monitoring port.\n        password: ""\n        # monitoring.basic_auth.username (string): Optional username for basic \n        # auth on the monitoring port.\n        username: ""\nolap_database:\n    # olap_database.cluster_name (string): The cluster name of the database\n    cluster_name: \'{cluster}\'\n    # olap_database.enable_data_replication (bool): If true, data replication \n    # is enabled.\n    enable_data_replication: false\n    # olap_database.replica_name (string): The replica name of the table in \n    # zookeeper\n    replica_name: \'{replica}\'\n    # olap_database.zoo_path (string): The path to the table name in \n    # zookeeper, used to set up data replication\n    zoo_path: /clickhouse/{installation}/{cluster}/tables/{shard}/{database}/{table}\nremote_execution:\n    # remote_execution.enable_executor_key_creation (bool): If enabled, UI \n    # will allow executor keys to be created.\n    enable_executor_key_creation: false\n    # remote_execution.enable_remote_exec (bool): If true, enable remote-exec. \n    # ** Enterprise only **\n    enable_remote_exec: true\n    # remote_execution.enable_user_owned_executors (bool): If enabled, users \n    # can register their own executors with the scheduler.\n    enable_user_owned_executors: false\n    # remote_execution.enable_workflows (bool): Whether to enable BuildBuddy \n    # workflows.\n    enable_workflows: false\n    # remote_execution.force_user_owned_darwin_executors (bool): If enabled, \n    # darwin actions will always run on user-owned executors.\n    force_user_owned_darwin_executors: false\nssl:\n    # ssl.cert_file (string): Path to a PEM encoded certificate file to use \n    # for TLS if not using ACME.\n    cert_file: ""\n    # ssl.client_ca_cert_file (string): Path to a PEM encoded certificate \n    # authority file used to issue client certificates for mTLS auth.\n    client_ca_cert_file: ""\n    # ssl.client_ca_key_file (string): Path to a PEM encoded certificate \n    # authority key file used to issue client certificates for mTLS auth.\n    client_ca_key_file: ""\n    # ssl.client_cert_lifespan (time.Duration): The duration client \n    # certificates are valid for. Ex: \'730h\' for one month. If not set, \n    # defaults to 100 years.\n    client_cert_lifespan: 876000h0m0s\n    # ssl.default_host (string): Host name to use for ACME generated cert if \n    # TLS request does not contain SNI.\n    default_host: ""\n    # ssl.enable_ssl (bool): Whether or not to enable SSL/TLS on gRPC \n    # connections (gRPCS).\n    enable_ssl: false\n    # ssl.host_whitelist ([]string): Cloud-Only\n    host_whitelist: []\n    # ssl.key_file (string): Path to a PEM encoded key file to use for TLS if \n    # not using ACME.\n    key_file: ""\n    # ssl.self_signed (bool): If true, a self-signed cert will be generated \n    # for TLS termination.\n    self_signed: false\n    # ssl.upgrade_insecure (bool): True if http requests should be redirected \n    # to https\n    upgrade_insecure: false\n    # ssl.use_acme (bool): Whether or not to automatically configure SSL certs \n    # using ACME. If ACME is enabled, cert_file and key_file should not be \n    # set.\n    use_acme: false\nstorage:\n    aws_s3:\n        # storage.aws_s3.bucket (string): The AWS S3 bucket to store files in.\n        bucket: ""\n        # storage.aws_s3.credentials_profile (string): A custom credentials \n        # profile to use.\n        credentials_profile: ""\n        # storage.aws_s3.disable_ssl (bool): Disables the use of SSL, useful \n        # for configuring the use of MinIO.\n        disable_ssl: false\n        # storage.aws_s3.endpoint (string): The AWS endpoint to use, useful \n        # for configuring the use of MinIO.\n        endpoint: ""\n        # storage.aws_s3.region (string): The AWS region.\n        region: ""\n        # storage.aws_s3.role_arn (string): The role ARN to use for web \n        # identity auth.\n        role_arn: ""\n        # storage.aws_s3.role_session_name (string): The role session name to \n        # use for web identity auth.\n        role_session_name: ""\n        # storage.aws_s3.s3_force_path_style (bool): Force path style urls for \n        # objects, useful for configuring the use of MinIO.\n        s3_force_path_style: false\n        # storage.aws_s3.static_credentials_id (string): Static credentials ID \n        # to use, useful for configuring the use of MinIO.\n        static_credentials_id: ""\n        # storage.aws_s3.static_credentials_secret (string): Static \n        # credentials secret to use, useful for configuring the use of MinIO.\n        static_credentials_secret: ""\n        # storage.aws_s3.static_credentials_token (string): Static credentials \n        # token to use, useful for configuring the use of MinIO.\n        static_credentials_token: ""\n        # storage.aws_s3.web_identity_token_file (string): The file path to \n        # the web identity token file.\n        web_identity_token_file: ""\n    azure:\n        # storage.azure.account_key (string): The key for the Azure storage \n        # account\n        account_key: ""\n        # storage.azure.account_name (string): The name of the Azure storage \n        # account\n        account_name: ""\n        # storage.azure.container_name (string): The name of the Azure storage \n        # container\n        container_name: ""\n    # storage.chunk_file_size_bytes (int): How many bytes to buffer in memory \n    # before flushing a chunk of build protocol data to disk.\n    chunk_file_size_bytes: 3000000\n    disk:\n        # storage.disk.root_directory (string): The root directory to store \n        # all blobs in, if using disk based storage.\n        root_directory: /tmp/buildbuddy\n        # storage.disk.use_v2_layout (bool): If enabled, files will be stored \n        # using the v2 layout. See disk_cache.MigrateToV2Layout for a \n        # description.\n        use_v2_layout: false\n    # storage.enable_chunked_event_logs (bool): If true, Event logs will be \n    # stored separately from the invocation proto in chunks.\n    enable_chunked_event_logs: false\n    gcs:\n        # storage.gcs.bucket (string): The name of the GCS bucket to store \n        # build artifact files in.\n        bucket: ""\n        # storage.gcs.credentials_file (string): A path to a JSON credentials \n        # file that will be used to authenticate to GCS.\n        credentials_file: ""\n        # storage.gcs.project_id (string): The Google Cloud project ID of the \n        # project owning the above credentials and GCS bucket.\n        project_id: ""\n    # storage.tempdir (string): Root directory for temporary files. Defaults \n    # to the OS-specific temp dir.\n    tempdir: /tmp\n    # storage.ttl_seconds (int): The time, in seconds, to keep invocations \n    # before deletion. 0 disables invocation deletion.\n    ttl_seconds: 0\n')))}l.isMDXComponent=!0;var c=["components"],d={toc:[]};function _(e){var t=e.components,n=(0,a.Z)(e,c);return(0,i.kt)("wrapper",(0,o.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'# Unstructured settings\n\n# app_directory (string): the directory containing app binary files to host\napp_directory: ""\n# auto_migrate_db (bool): If true, attempt to automigrate the db when \n# connecting\nauto_migrate_db: true\n# auto_migrate_db_and_exit (bool): If true, attempt to automigrate the db when \n# connecting, then exit the program.\nauto_migrate_db_and_exit: false\n# cache_stats_finalization_delay (time.Duration): The time allowed for all \n# metrics collectors across all apps to flush their local cache stats to the \n# backing storage, before finalizing stats in the DB.\ncache_stats_finalization_delay: 500ms\n# cleanup_interval (time.Duration): How often the janitor cleanup tasks will \n# run\ncleanup_interval: 10m0s\n# cleanup_workers (int): How many cleanup tasks to run\ncleanup_workers: 1\n# disable_ga (bool): If true; ga will be disabled\ndisable_ga: false\n# disable_telemetry (bool): If true; telemetry will be disabled\ndisable_telemetry: false\n# drop_invocation_pk_cols (bool): If true, attempt to drop invocation PK cols\ndrop_invocation_pk_cols: false\n# enable_cache_delete_api (bool): If true, enable access to cache delete API.\nenable_cache_delete_api: false\n# exit_when_ready (bool): If set, the app will exit as soon as it becomes \n# ready (useful for migrations)\nexit_when_ready: false\n# grpc_port (int): The port to listen for gRPC traffic on\ngrpc_port: 1985\n# grpcs_port (int): The port to listen for gRPCS traffic on\ngrpcs_port: 1986\n# internal_grpc_port (int): The port to listen for internal gRPC traffic on\ninternal_grpc_port: 1987\n# internal_grpcs_port (int): The port to listen for internal gRPCS traffic on\ninternal_grpcs_port: 1988\n# internal_http_port (int): The port to listen for internal HTTP traffic\ninternal_http_port: 0\n# js_entry_point_path (string): Absolute URL path of the app JS entry point\njs_entry_point_path: /app/app_bundle/app.js?hash={APP_BUNDLE_HASH}\n# listen (string): The interface to listen on (default: 0.0.0.0)\nlisten: 0.0.0.0\n# log_deletion_errors (bool): If true; log errors when ttl-deleting expired \n# data\nlog_deletion_errors: false\n# max_shutdown_duration (time.Duration): Time to wait for shutdown\nmax_shutdown_duration: 25s\n# migrate_disk_cache_to_v2_and_exit (bool): If true, attempt to migrate disk \n# cache to v2 layout.\nmigrate_disk_cache_to_v2_and_exit: false\n# monitoring_port (int): The port to listen for monitoring traffic on\nmonitoring_port: 9090\n# port (int): The port to listen for HTTP traffic on\nport: 8080\n# redis_command_buffer_flush_period (time.Duration): How long to wait between \n# flushing buffered redis commands. Setting this to 0 will disable buffering \n# at the cost of higher redis QPS.\nredis_command_buffer_flush_period: 250ms\n# server_type (string): The server type to match on health checks\nserver_type: buildbuddy-server\n# shutdown_lameduck_duration (time.Duration): If set, the server will be \n# marked unready but not run shutdown functions until this period passes.\nshutdown_lameduck_duration: 0s\n# ssl_port (int): The port to listen for HTTPS traffic on\nssl_port: 8081\n# static_directory (string): the directory containing static files to host\nstatic_directory: ""\n# telemetry_endpoint (string): The telemetry endpoint to use\ntelemetry_endpoint: grpcs://t.buildbuddy.io:443\n# telemetry_interval (time.Duration): How often telemetry data will be \n# reported\ntelemetry_interval: 24h0m0s\n# telemetry_port (int): The port on which to listen for telemetry events\ntelemetry_port: 9099\n# verbose_telemetry_client (bool): If true; print telemetry client information\nverbose_telemetry_client: false\n# verbose_telemetry_server (bool): If true; print telemetry server information\nverbose_telemetry_server: false\n# zone_override (string): A value that will override the auto-detected zone. \n# Ignored if empty\nzone_override: ""\n\n# Structured settings\n\napi:\n    # api.api_key (string): The default API key to use for on-prem enterprise \n    # deploys with a single organization/group. **DEPRECATED** Manual API key \n    # specification is no longer supported; to retrieve specific API keys \n    # programmatically, please use the API key table. This field will still \n    # specify an API key to redact in case a manual API key was specified when \n    # buildbuddy was first set up.\n    api_key: ""\n    # api.enable_api (bool): Whether or not to enable the BuildBuddy API.\n    enable_api: true\n    # api.enable_cache (bool): Whether or not to enable the API cache.\n    enable_cache: false\napp:\n    # app.add_user_to_domain_group (bool): Cloud-Only\n    add_user_to_domain_group: false\n    # app.admin_only_create_group (bool): If true, only admins of an existing \n    # group can create a new groups.\n    admin_only_create_group: false\n    # app.build_buddy_url (URL): The external URL where your BuildBuddy \n    # instance can be found.\n    build_buddy_url: http://localhost:8080\n    # app.cache_api_url (URL): Overrides the default remote cache protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    cache_api_url: ""\n    # app.code_editor_enabled (bool): If set, code editor functionality will \n    # be enabled.\n    code_editor_enabled: false\n    # app.create_group_per_user (bool): Cloud-Only\n    create_group_per_user: false\n    # app.default_redis_target (string): A Redis target for storing remote \n    # shared state. To ease migration, the redis target from the remote \n    # execution config will be used if this value is not specified.\n    default_redis_target: ""\n    default_sharded_redis:\n        # app.default_sharded_redis.password (string): Redis password\n        password: ""\n        # app.default_sharded_redis.shards ([]string): Ordered list of Redis \n        # shard addresses.\n        shards: []\n        # app.default_sharded_redis.username (string): Redis username\n        username: ""\n    # app.default_to_dense_mode (bool): Enables the dense UI mode by default.\n    default_to_dense_mode: false\n    # app.disable_cert_config (bool): If true, the certificate based auth \n    # option will not be shown in the config widget.\n    disable_cert_config: false\n    # app.enable_execution_trends (bool): If enabled, fill execution trend \n    # stats in GetTrendResponse\n    enable_execution_trends: false\n    # app.enable_grpc_metrics_by_group_id (bool): If enabled, grpc metrics by \n    # group ID will be recorded\n    enable_grpc_metrics_by_group_id: false\n    # app.enable_invocation_stat_percentiles (bool): If enabled, provide \n    # percentile breakdowns for invocation stats in GetTrendResponse\n    enable_invocation_stat_percentiles: false\n    # app.enable_prometheus_histograms (bool): If true, collect prometheus \n    # histograms for all RPCs\n    enable_prometheus_histograms: true\n    # app.enable_quota_management (bool): If set, quota management will be \n    # enabled\n    enable_quota_management: false\n    # app.enable_read_from_olap_db (bool): If enabled, read from OLAP DB\n    enable_read_from_olap_db: false\n    # app.enable_read_target_statuses_from_olap_db (bool): If enabled, read \n    # target statuses from OLAP DB\n    enable_read_target_statuses_from_olap_db: false\n    # app.enable_secret_service (bool): If set, secret service will be enabled\n    enable_secret_service: false\n    # app.enable_structured_logging (bool): If true, log messages will be \n    # json-formatted.\n    enable_structured_logging: false\n    # app.enable_target_tracking (bool): Cloud-Only\n    enable_target_tracking: false\n    # app.enable_write_executions_to_olap_db (bool): If enabled, complete \n    # Executions will be flushed to OLAP DB\n    enable_write_executions_to_olap_db: false\n    # app.enable_write_test_target_statuses_to_olap_db (bool): If enabled, \n    # test target statuses will be flushed to OLAP DB\n    enable_write_test_target_statuses_to_olap_db: false\n    # app.enable_write_to_olap_db (bool): If enabled, complete invocations \n    # will be flushed to OLAP DB\n    enable_write_to_olap_db: false\n    # app.events_api_url (URL): Overrides the default build event protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    events_api_url: ""\n    # app.expanded_suggestions_enabled (bool): If set, enable more build \n    # suggestions in the UI.\n    expanded_suggestions_enabled: false\n    # app.global_filter_enabled (bool): If set, the global filter will be \n    # enabled in the UI.\n    global_filter_enabled: true\n    # app.grpc_max_recv_msg_size_bytes (int): Configures the max GRPC receive \n    # message size [bytes]\n    grpc_max_recv_msg_size_bytes: 50000000\n    # app.grpc_over_http_port_enabled (bool): Cloud-Only\n    grpc_over_http_port_enabled: false\n    # app.ignore_forced_tracing_header (bool): If set, we will not honor the \n    # forced tracing header.\n    ignore_forced_tracing_header: false\n    # app.log_enable_gcp_logging_format (bool): If true, the output structured \n    # logs will be compatible with format expected by GCP Logging.\n    log_enable_gcp_logging_format: false\n    # app.log_error_stack_traces (bool): If true, stack traces will be printed \n    # for errors that have them.\n    log_error_stack_traces: false\n    # app.log_gcp_log_id (string): The log ID to log to in GCP (if any).\n    log_gcp_log_id: ""\n    # app.log_gcp_project_id (string): The project ID to log to in GCP (if \n    # any).\n    log_gcp_project_id: ""\n    # app.log_include_short_file_name (bool): If true, log messages will \n    # include shortened originating file name.\n    log_include_short_file_name: false\n    # app.log_level (string): The desired log level. Logs with a level >= this \n    # level will be emitted. One of {\'fatal\', \'error\', \'warn\', \'info\', \n    # \'debug\'}\n    log_level: info\n    # app.no_default_user_group (bool): Cloud-Only\n    no_default_user_group: false\n    # app.region (string): The region in which the app is running.\n    region: ""\n    # app.remote_execution_api_url (URL): Overrides the default remote \n    # execution protocol gRPC address shown by BuildBuddy on the configuration \n    # screen.\n    remote_execution_api_url: ""\n    # app.test_grid_v2_enabled (bool): Whether to enable test grid V2\n    test_grid_v2_enabled: true\n    # app.test_output_manifests_enabled (bool): If set, the target page will \n    # render the contents of test output zips.\n    test_output_manifests_enabled: true\n    # app.trace_fraction (float64): Fraction of requests to sample for \n    # tracing.\n    trace_fraction: 0\n    # app.trace_fraction_overrides ([]string): Tracing fraction override based \n    # on name in format name=fraction.\n    trace_fraction_overrides: []\n    # app.trace_jaeger_collector (string): Address of the Jager collector \n    # endpoint where traces will be sent.\n    trace_jaeger_collector: ""\n    # app.trace_project_id (string): Optional GCP project ID to export traces \n    # to. If not specified, determined from default credentials or metadata \n    # server if running on GCP.\n    trace_project_id: ""\n    # app.trace_service_name (string): Name of the service to associate with \n    # traces.\n    trace_service_name: ""\n    # app.usage_enabled (bool): If set, the usage page will be enabled in the \n    # UI.\n    usage_enabled: false\n    # app.usage_start_date (string): If set, usage data will only be viewable \n    # on or after this timestamp. Specified in RFC3339 format, like \n    # 2021-10-01T00:00:00Z\n    usage_start_date: ""\n    # app.usage_tracking_enabled (bool): If set, enable usage data collection.\n    usage_tracking_enabled: false\n    # app.user_management_enabled (bool): If set, the user management page \n    # will be enabled in the UI.\n    user_management_enabled: true\n    # app.user_owned_keys_enabled (bool): If set, enable the UI controls for \n    # user-owned API keys.\n    user_owned_keys_enabled: false\nauth:\n    # auth.admin_group_id (string): ID of a group whose members can perform \n    # actions only accessible to server admins.\n    admin_group_id: ""\n    # auth.api_key_group_cache_ttl (time.Duration): TTL for API Key to Group \n    # caching. Set to \'0\' to disable cache.\n    api_key_group_cache_ttl: 5m0s\n    # auth.disable_refresh_token (bool): If true, the offline_access scope \n    # which requests refresh tokens will not be requested.\n    disable_refresh_token: false\n    # auth.enable_anonymous_usage (bool): If true, unauthenticated build \n    # uploads will still be allowed but won\'t be associated with your \n    # organization.\n    enable_anonymous_usage: false\n    # auth.enable_self_auth (bool): If true, enables a single user login via \n    # an oauth provider on the buildbuddy server. Recommend use only when \n    # server is behind a firewall; this option may allow anyone with access to \n    # the webpage admin rights to your buildbuddy installation. ** Enterprise \n    # only **\n    enable_self_auth: false\n    # auth.force_approval (bool): If true, when a user doesn\'t have a session \n    # (first time logging in, or manually logged out) force the auth provider \n    # to show the consent screen allowing the user to select an account if \n    # they have multiple. This isn\'t supported by all auth providers.\n    force_approval: false\n    # auth.https_only_cookies (bool): If true, cookies will only be set over \n    # https connections.\n    https_only_cookies: false\n    # auth.jwt_claims_cache_ttl (time.Duration): TTL for JWT string to parsed \n    # claims caching. Set to \'0\' to disable cache.\n    jwt_claims_cache_ttl: 15s\n    # auth.jwt_key (string): The key to use when signing JWT tokens.\n    jwt_key: set_the_jwt_in_config\n    # auth.oauth_providers ([]auth.OauthProvider): The list of oauth providers \n    # to use to authenticate.\n    oauth_providers: []\n    # For example:\n    # - issuer_url: "" # The issuer URL of this OIDC Provider. (type: string)\n    #   client_id: "" # The oauth client ID. (type: string)\n    #   client_secret: "" # The oauth client secret. (type: string)\n    #   slug: "" # The slug of this OIDC Provider. (type: string)\n\n    saml:\n        # auth.saml.cert_file (string): Path to a PEM encoded certificate file \n        # used for SAML auth.\n        cert_file: ""\n        # auth.saml.key_file (string): Path to a PEM encoded certificate key \n        # file used for SAML auth.\n        key_file: ""\nbuild_event_proxy:\n    # build_event_proxy.buffer_size (int): The number of build events to \n    # buffer locally when proxying build events.\n    buffer_size: 100\n    # build_event_proxy.hosts ([]string): The list of hosts to pass build \n    # events onto.\n    hosts: []\ncache:\n    client:\n        # cache.client.enable_upload_compression (bool): If true, enable \n        # compression of uploads to remote caches\n        enable_upload_compression: false\n    # cache.detailed_stats_enabled (bool): Whether to enable detailed stats \n    # recording for all cache requests.\n    detailed_stats_enabled: false\n    disk:\n        # cache.disk.partition_mappings ([]disk.PartitionMapping)\n        partition_mappings: []\n        # For example:\n        # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n\n        # cache.disk.partitions ([]disk.Partition)\n        partitions: []\n        # For example:\n        # - id: "" # The ID of the partition. (type: string)\n        #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n\n        # cache.disk.root_directory (string): The root directory to store all \n        # blobs in, if using disk based storage.\n        root_directory: ""\n        # cache.disk.use_v2_layout (bool): If enabled, files will be stored \n        # using the v2 layout. See disk_cache.MigrateToV2Layout for a \n        # description.\n        use_v2_layout: false\n    distributed_cache:\n        # cache.distributed_cache.cluster_size (int): The total number of \n        # nodes in this cluster. Required for health checking. ** Enterprise \n        # only **\n        cluster_size: 0\n        # cache.distributed_cache.enable_local_compression_lookup (bool): If \n        # enabled, checks the local cache for compression support. If not set, \n        # distributed compression defaults to off.\n        enable_local_compression_lookup: false\n        # cache.distributed_cache.enable_local_writes (bool): If enabled, \n        # shortcuts distributed writes that belong to the local shard to local \n        # cache instead of making an RPC.\n        enable_local_writes: false\n        # cache.distributed_cache.group_name (string): A unique name for this \n        # distributed cache group. ** Enterprise only **\n        group_name: ""\n        # cache.distributed_cache.listen_addr (string): The address to listen \n        # for local BuildBuddy distributed cache traffic on.\n        listen_addr: ""\n        # cache.distributed_cache.nodes ([]string): The hardcoded list of peer \n        # distributed cache nodes. If this is set, redis_target will be \n        # ignored. ** Enterprise only **\n        nodes: []\n        # cache.distributed_cache.redis_target (string): A redis target for \n        # improved Caching/RBE performance. Target can be provided as either a \n        # redis connection URI or a host:port pair. URI schemas supported: \n        # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n        # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise \n        # only **\n        redis_target: ""\n        # cache.distributed_cache.replication_factor (int): How many total \n        # servers the data should be replicated to. Must be >= 1. ** \n        # Enterprise only **\n        replication_factor: 0\n    # cache.enable_tree_caching (bool): If true, cache GetTree responses (full \n    # and partial)\n    enable_tree_caching: true\n    gcs:\n        # cache.gcs.bucket (string): The name of the GCS bucket to store cache \n        # files in.\n        bucket: ""\n        # cache.gcs.credentials_file (string): A path to a JSON credentials \n        # file that will be used to authenticate to GCS.\n        credentials_file: ""\n        # cache.gcs.project_id (string): The Google Cloud project ID of the \n        # project owning the above credentials and GCS bucket.\n        project_id: ""\n        # cache.gcs.ttl_days (int64): The period after which cache files \n        # should be TTLd. Disabled if 0.\n        ttl_days: 0\n    # cache.in_memory (bool): Whether or not to use the in_memory cache.\n    in_memory: false\n    # cache.max_size_bytes (int64): How big to allow the cache to be (in \n    # bytes).\n    max_size_bytes: 10000000000\n    # cache.memcache_targets ([]string): Deprecated. Use Redis Target instead.\n    memcache_targets: []\n    # cache.migration (migration_cache.MigrationConfig): Config to specify the \n    # details of a cache migration\n    migration:\n        src: null # (type: migration_cache.CacheConfig)\n        # For example:\n        #     disk: null # (type: migration_cache.DiskCacheConfig)\n        #     # For example:\n        #     #     root_directory: "" # (type: string)\n        #     #     partitions: [] # (type: []disk.Partition)\n        #     #     # For example:\n        #     #     # - id: "" # The ID of the partition. (type: string)\n        #     #     #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #     #     \n        #     #     partition_mappings: [] # (type: []disk.PartitionMapping)\n        #     #     # For example:\n        #     #     # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #     #     #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #     #     #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n        #     #     \n        #     #     use_v2_layout: false # (type: bool)\n        #     #     \n        #     \n        #     pebble: null # (type: migration_cache.PebbleCacheConfig)\n        #     # For example:\n        #     #     name: "" # (type: string)\n        #     #     root_directory: "" # (type: string)\n        #     #     partitions: [] # (type: []disk.Partition)\n        #     #     # For example:\n        #     #     # - id: "" # The ID of the partition. (type: string)\n        #     #     #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #     #     \n        #     #     partition_mappings: [] # (type: []disk.PartitionMapping)\n        #     #     # For example:\n        #     #     # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #     #     #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #     #     #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n        #     #     \n        #     #     max_size_bytes: 0 # (type: int64)\n        #     #     block_cache_size_bytes: 0 # (type: int64)\n        #     #     max_inline_file_size_bytes: 0 # (type: int64)\n        #     #     atime_update_threshold: null # (type: time.Duration)\n        #     #     atime_buffer_size: null # (type: int)\n        #     #     min_eviction_age: null # (type: time.Duration)\n        #     #     min_bytes_auto_zstd_compression: 0 # (type: int64)\n        #     #     \n        #     \n\n        dest: null # (type: migration_cache.CacheConfig)\n        # For example:\n        #     disk: null # (type: migration_cache.DiskCacheConfig)\n        #     # For example:\n        #     #     root_directory: "" # (type: string)\n        #     #     partitions: [] # (type: []disk.Partition)\n        #     #     # For example:\n        #     #     # - id: "" # The ID of the partition. (type: string)\n        #     #     #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #     #     \n        #     #     partition_mappings: [] # (type: []disk.PartitionMapping)\n        #     #     # For example:\n        #     #     # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #     #     #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #     #     #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n        #     #     \n        #     #     use_v2_layout: false # (type: bool)\n        #     #     \n        #     \n        #     pebble: null # (type: migration_cache.PebbleCacheConfig)\n        #     # For example:\n        #     #     name: "" # (type: string)\n        #     #     root_directory: "" # (type: string)\n        #     #     partitions: [] # (type: []disk.Partition)\n        #     #     # For example:\n        #     #     # - id: "" # The ID of the partition. (type: string)\n        #     #     #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #     #     \n        #     #     partition_mappings: [] # (type: []disk.PartitionMapping)\n        #     #     # For example:\n        #     #     # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #     #     #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #     #     #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n        #     #     \n        #     #     max_size_bytes: 0 # (type: int64)\n        #     #     block_cache_size_bytes: 0 # (type: int64)\n        #     #     max_inline_file_size_bytes: 0 # (type: int64)\n        #     #     atime_update_threshold: null # (type: time.Duration)\n        #     #     atime_buffer_size: null # (type: int)\n        #     #     min_eviction_age: null # (type: time.Duration)\n        #     #     min_bytes_auto_zstd_compression: 0 # (type: int64)\n        #     #     \n        #     \n\n        double_read_percentage: 0 # (type: float64)\n        log_not_found_errors: false # (type: bool)\n        copy_chan_buffer_size: 0 # (type: int)\n        copy_chan_full_warning_interval_min: 0 # (type: int64)\n        max_copies_per_sec: 0 # (type: int)\n    pebble:\n        # cache.pebble.activeKeyVersion (int64): The key version new data will \n        # be written with\n        activeKeyVersion: 0\n        # cache.pebble.atime_buffer_size (int): Buffer up to this many atime \n        # updates in a channel before dropping atime updates\n        atime_buffer_size: 100000\n        # cache.pebble.atime_update_threshold (time.Duration): Don\'t update \n        # atime if it was updated more recently than this\n        atime_update_threshold: 10m0s\n        # cache.pebble.background_repair_frequency (time.Duration): How \n        # frequently to run period background repair tasks.\n        background_repair_frequency: 24h0m0s\n        # cache.pebble.background_repair_qps_limit (int): QPS limit for \n        # background repair modifications.\n        background_repair_qps_limit: 100\n        # cache.pebble.block_cache_size_bytes (int64): How much ram to give \n        # the block cache\n        block_cache_size_bytes: 1000000000\n        # cache.pebble.delete_ac_entries_older_than (time.Duration): If set, \n        # the background repair will delete AC entries older than this time.\n        delete_ac_entries_older_than: 0s\n        # cache.pebble.dir_deletion_delay (time.Duration): How old directories \n        # must be before being eligible for deletion when empty\n        dir_deletion_delay: 1h0m0s\n        # cache.pebble.force_calculate_metadata (bool): If set, partition size \n        # and counts will be calculated even if cached information is \n        # available.\n        force_calculate_metadata: false\n        # cache.pebble.force_compaction (bool): If set, compact the DB when \n        # it\'s created\n        force_compaction: false\n        # cache.pebble.max_inline_file_size_bytes (int64): Files smaller than \n        # this may be inlined directly into pebble\n        max_inline_file_size_bytes: 1024\n        # cache.pebble.min_bytes_auto_zstd_compression (int64): Blobs larger \n        # than this will be zstd compressed before written to disk.\n        min_bytes_auto_zstd_compression: 0\n        # cache.pebble.min_eviction_age (time.Duration): Don\'t evict anything \n        # unless it\'s been idle for at least this long\n        min_eviction_age: 6h0m0s\n        # cache.pebble.name (string): The name used in reporting cache metrics \n        # and status.\n        name: pebble_cache\n        # cache.pebble.orphan_delete_dry_run (bool): If set, log orphaned \n        # files instead of deleting them\n        orphan_delete_dry_run: true\n        # cache.pebble.partition_mappings ([]disk.PartitionMapping)\n        partition_mappings: []\n        # For example:\n        # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n\n        # cache.pebble.partitions ([]disk.Partition)\n        partitions: []\n        # For example:\n        # - id: "" # The ID of the partition. (type: string)\n        #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n\n        # cache.pebble.root_directory (string): The root directory to store \n        # the database in.\n        root_directory: ""\n        # cache.pebble.sample_pool_size (int): How many deletion candidates to \n        # maintain between evictions\n        sample_pool_size: 500\n        # cache.pebble.samples_per_eviction (int): How many records to sample \n        # on each eviction\n        samples_per_eviction: 20\n        # cache.pebble.scan_for_orphaned_files (bool): If true, scan for \n        # orphaned files\n        scan_for_orphaned_files: false\n        # cache.pebble.warn_about_leaks (bool): If set, warn about leaked DB \n        # handles\n        warn_about_leaks: true\n    raft:\n        # cache.raft.atime_buffer_size (int): Buffer up to this many atime \n        # updates in a channel before dropping atime updates\n        atime_buffer_size: 1000\n        # cache.raft.atime_update_threshold (time.Duration): Don\'t update \n        # atime if it was updated more recently than this\n        atime_update_threshold: 10m0s\n        # cache.raft.atime_write_batch_size (int): Buffer this many writes \n        # before writing atime data\n        atime_write_batch_size: 100\n        # cache.raft.clear_cache_on_startup (bool): If set, remove all raft + \n        # cache data on start\n        clear_cache_on_startup: false\n        # cache.raft.dead_replica_timeout (time.Duration): After this time, \n        # consider a node dead\n        dead_replica_timeout: 5m0s\n        # cache.raft.driver_poll_interval (time.Duration): Poll the cluster \n        # for moves/replacements this often\n        driver_poll_interval: 10s\n        # cache.raft.driver_startup_delay (time.Duration): Don\'t allow driver \n        # to propose any changes until this window has passed\n        driver_startup_delay: 1m0s\n        # cache.raft.enable_moving_replicas (bool): If set, allow moving \n        # replicas between nodes\n        enable_moving_replicas: true\n        # cache.raft.enable_replacing_replicas (bool): If set, allow replacing \n        # dead / down replicas\n        enable_replacing_replicas: true\n        # cache.raft.enable_splitting_replicas (bool): If set, allow splitting \n        # oversize replicas\n        enable_splitting_replicas: true\n        # cache.raft.grpc_addr (string): The address to listen for internal \n        # API traffic on. Ex. \'1993\'\n        grpc_addr: ""\n        # cache.raft.http_addr (string): The address to listen for HTTP raft \n        # traffic. Ex. \'1992\'\n        http_addr: ""\n        # cache.raft.join ([]string): The list of nodes to use when joining \n        # clusters Ex. \'1.2.3.4:1991,2.3.4.5:1991...\'\n        join: []\n        # cache.raft.listen_addr (string): The address to listen for local \n        # gossip traffic on. Ex. \'localhost:1991\n        listen_addr: ""\n        # cache.raft.partition_mappings ([]disk.PartitionMapping)\n        partition_mappings: []\n        # For example:\n        # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n\n        # cache.raft.partition_usage_delta_bytes_threshold (int): Gossip \n        # partition usage information if it has changed by more than this \n        # amount since the last gossip.\n        partition_usage_delta_bytes_threshold: 100000000\n        # cache.raft.partitions ([]disk.Partition)\n        partitions: []\n        # For example:\n        # - id: "" # The ID of the partition. (type: string)\n        #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n\n        # cache.raft.replica_split_size_bytes (int64): Split replicas after \n        # they reach this size\n        replica_split_size_bytes: 20000000\n        # cache.raft.root_directory (string): The root directory to use for \n        # storing cached data.\n        root_directory: ""\n        # cache.raft.sample_pool_size (int): How many deletion candidates to \n        # maintain between evictions\n        sample_pool_size: 500\n        # cache.raft.samples_per_eviction (int): How many records to sample on \n        # each eviction\n        samples_per_eviction: 20\n    redis:\n        # cache.redis.max_value_size_bytes (int64): The maximum value size to \n        # cache in redis (in bytes).\n        max_value_size_bytes: 10000000\n        # cache.redis.redis_target (string): A redis target for improved \n        # Caching/RBE performance. Target can be provided as either a redis \n        # connection URI or a host:port pair. URI schemas supported: \n        # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n        # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise \n        # only **\n        redis_target: ""\n        sharded:\n            # cache.redis.sharded.password (string): Redis password\n            password: ""\n            # cache.redis.sharded.shards ([]string): Ordered list of Redis \n            # shard addresses.\n            shards: []\n            # cache.redis.sharded.username (string): Redis username\n            username: ""\n    # cache.redis_target (string): A redis target for improved Caching/RBE \n    # performance. Target can be provided as either a redis connection URI or \n    # a host:port pair. URI schemas supported: \n    # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n    # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise only \n    # **\n    redis_target: ""\n    s3:\n        # cache.s3.bucket (string): The AWS S3 bucket to store files in.\n        bucket: ""\n        # cache.s3.credentials_profile (string): A custom credentials profile \n        # to use.\n        credentials_profile: ""\n        # cache.s3.disable_ssl (bool): Disables the use of SSL, useful for \n        # configuring the use of MinIO.\n        disable_ssl: false\n        # cache.s3.endpoint (string): The AWS endpoint to use, useful for \n        # configuring the use of MinIO.\n        endpoint: ""\n        # cache.s3.region (string): The AWS region.\n        region: ""\n        # cache.s3.role_arn (string): The role ARN to use for web identity \n        # auth.\n        role_arn: ""\n        # cache.s3.role_session_name (string): The role session name to use \n        # for web identity auth.\n        role_session_name: ""\n        # cache.s3.s3_force_path_style (bool): Force path style urls for \n        # objects, useful for configuring the use of MinIO.\n        s3_force_path_style: false\n        # cache.s3.static_credentials_id (string): Static credentials ID to \n        # use, useful for configuring the use of MinIO.\n        static_credentials_id: ""\n        # cache.s3.static_credentials_secret (string): Static credentials \n        # secret to use, useful for configuring the use of MinIO.\n        static_credentials_secret: ""\n        # cache.s3.static_credentials_token (string): Static credentials token \n        # to use, useful for configuring the use of MinIO.\n        static_credentials_token: ""\n        # cache.s3.ttl_days (int64): The period after which cache files should \n        # be TTLd. Disabled if 0.\n        ttl_days: 0\n        # cache.s3.web_identity_token_file (string): The file path to the web \n        # identity token file.\n        web_identity_token_file: ""\n    # cache.tree_cache_seed (string): If set, hash this with digests before \n    # caching / reading from tree cache\n    tree_cache_seed: treecache-07012022\n    # cache.zstd_transcoding_enabled (bool): Whether to accept requests to \n    # read/write zstd-compressed blobs, compressing/decompressing \n    # outgoing/incoming blobs on the fly.\n    zstd_transcoding_enabled: true\ndatabase:\n    # database.advanced_data_source (db.AdvancedConfig): Alternative to the \n    # database.data_source flag that allows finer control over database \n    # settings as well as allowing use of AWS IAM credentials. For most users, \n    # database.data_source is a simpler configuration method.\n    advanced_data_source:\n        driver: "" # The driver to use: one of sqlite3 or mysql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.advanced_read_replica (db.AdvancedConfig): Advanced alternative \n    # to database.read_replica. Refer to database.advanced for more \n    # information.\n    advanced_read_replica:\n        driver: "" # The driver to use: one of sqlite3 or mysql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.conn_max_lifetime_seconds (int): The maximum lifetime of a \n    # connection to the db\n    conn_max_lifetime_seconds: 0\n    # database.data_source (string): The SQL database to connect to, specified \n    # as a connection string.\n    data_source: sqlite3:///tmp/buildbuddy.db\n    # database.log_queries (bool): If true, log all queries\n    log_queries: false\n    # database.max_idle_conns (int): The maximum number of idle connections to \n    # maintain to the db\n    max_idle_conns: 0\n    # database.max_open_conns (int): The maximum number of open connections to \n    # maintain to the db\n    max_open_conns: 0\n    # database.read_replica (string): A secondary, read-only SQL database to \n    # connect to, specified as a connection string.\n    read_replica: ""\n    # database.slow_query_threshold (time.Duration): Queries longer than this \n    # duration will be logged with a \'Slow SQL\' warning.\n    slow_query_threshold: 500ms\n    # database.stats_poll_interval (time.Duration): How often to poll the DB \n    # client for connection stats (default: \'5s\').\n    stats_poll_interval: 5s\nexecutor:\n    # executor.default_image (string): The default docker image to use to warm \n    # up executors or if no platform property is set. Ex: \n    # gcr.io/flame-public/executor-docker-default:enterprise-v1.5.4\n    default_image: gcr.io/flame-public/executor-docker-default:enterprise-v1.6.0\n    # executor.default_isolation_type (string): The default workload isolation \n    # type when no type is specified in an action. If not set, we use the \n    # first of the following that is set: docker, firecracker, podman, or \n    # barerunner\n    default_isolation_type: ""\n    # executor.default_xcode_version (string): Sets the default Xcode version \n    # number to use if an action doesn\'t specify one. If not set, \n    # /Applications/Xcode.app/ is used.\n    default_xcode_version: ""\n    # executor.docker_socket (string): If set, run execution commands in \n    # docker using the provided socket.\n    docker_socket: ""\n    # executor.enable_bare_runner (bool): Enables running execution commands \n    # directly on the host without isolation.\n    enable_bare_runner: false\n    # executor.enable_firecracker (bool): Enables running execution commands \n    # inside of firecracker VMs\n    enable_firecracker: false\n    # executor.enable_podman (bool): Enables running execution commands inside \n    # podman container.\n    enable_podman: false\n    # executor.enable_sandbox (bool): Enables running execution commands \n    # inside of sandbox-exec.\n    enable_sandbox: false\n    # executor.enable_vfs (bool): Whether FUSE based filesystem is enabled.\n    enable_vfs: false\n    # executor.extra_env_vars ([]string): Additional environment variables to \n    # pass to remotely executed actions. i.e. MY_ENV_VAR=foo\n    extra_env_vars: []\n    # executor.memory_bytes (int64): Optional maximum memory to allocate to \n    # execution tasks (approximate). Cannot set both this option and the \n    # SYS_MEMORY_BYTES env var.\n    memory_bytes: 0\n    # executor.millicpu (int64): Optional maximum CPU milliseconds to allocate \n    # to execution tasks (approximate). Cannot set both this option and the \n    # SYS_MILLICPU env var.\n    millicpu: 0\ngithub:\n    # github.access_token (string): The GitHub access token used to post \n    # GitHub commit statuses. ** Enterprise only **\n    access_token: ""\n    # github.client_id (string): The client ID of your GitHub Oauth App. ** \n    # Enterprise only **\n    client_id: ""\n    # github.client_secret (string): The client secret of your GitHub Oauth \n    # App. ** Enterprise only **\n    client_secret: ""\n    # github.status_name_suffix (string): Suffix to be appended to all \n    # reported GitHub status names. Useful for differentiating BuildBuddy \n    # deployments. For example: \'(dev)\' ** Enterprise only **\n    status_name_suffix: ""\n    # github.status_per_test_target (bool): If true, report status per test \n    # target. ** Enterprise only **\n    status_per_test_target: false\nintegrations:\n    invocation_upload:\n        # integrations.invocation_upload.enabled (bool): Whether to upload \n        # webhook data to the webhook URL configured per-Group. ** Enterprise \n        # only **\n        enabled: false\n        # integrations.invocation_upload.gcs_credentials (string): Credentials \n        # JSON for the Google service account used to authenticate when GCS is \n        # used as the invocation upload target. ** Enterprise only **\n        gcs_credentials: ""\n    slack:\n        # integrations.slack.webhook_url (string): A Slack webhook url to post \n        # build update messages to.\n        webhook_url: ""\nkeystore:\n    gcp:\n        # keystore.gcp.credentials_file (string): A path to a gcp JSON \n        # credentials file that will be used to authenticate.\n        credentials_file: ""\n    # keystore.master_key_uri (string): The master key URI (see tink docs for \n    # example)\n    master_key_uri: ""\nmonitoring:\n    basic_auth:\n        # monitoring.basic_auth.password (string): Optional password for basic \n        # auth on the monitoring port.\n        password: ""\n        # monitoring.basic_auth.username (string): Optional username for basic \n        # auth on the monitoring port.\n        username: ""\nolap_database:\n    # olap_database.auto_migrate_db (bool): If true, attempt to automigrate \n    # the db when connecting\n    auto_migrate_db: true\n    # olap_database.cluster_name (string): The cluster name of the database\n    cluster_name: \'{cluster}\'\n    # olap_database.conn_max_lifetime (time.Duration): The maximum lifetime of \n    # a connection to clickhouse\n    conn_max_lifetime: 0s\n    # olap_database.data_source (string): The clickhouse database to connect \n    # to, specified a a connection string\n    data_source: ""\n    # olap_database.enable_data_replication (bool): If true, data replication \n    # is enabled.\n    enable_data_replication: false\n    # olap_database.max_idle_conns (int): The maximum number of idle \n    # connections to maintain to the db\n    max_idle_conns: 0\n    # olap_database.max_open_conns (int): The maximum number of open \n    # connections to maintain to the db\n    max_open_conns: 0\n    # olap_database.replica_name (string): The replica name of the table in \n    # zookeeper\n    replica_name: \'{replica}\'\n    # olap_database.zoo_path (string): The path to the table name in \n    # zookeeper, used to set up data replication\n    zoo_path: /clickhouse/{installation}/{cluster}/tables/{shard}/{database}/{table}\norg:\n    # org.domain (string): Your organization\'s email domain. If this is set, \n    # only users with email addresses in this domain will be able to register \n    # for a BuildBuddy account.\n    domain: ""\n    # org.name (string): The name of your organization, which is displayed on \n    # your organization\'s build history.\n    name: Organization\nregistry:\n    # registry.enabled (bool): Whether to enable registry services\n    enabled: false\n    # registry.image_converter_backend (string): gRPC endpoint of the image \n    # converter service\n    image_converter_backend: ""\nremote_execution:\n    # remote_execution.default_pool_name (string): The default executor pool \n    # to use if one is not specified.\n    default_pool_name: ""\n    # remote_execution.enable_action_merging (bool): If enabled, identical \n    # actions being executed concurrently are merged into a single execution.\n    enable_action_merging: true\n    # remote_execution.enable_executor_key_creation (bool): If enabled, UI \n    # will allow executor keys to be created.\n    enable_executor_key_creation: false\n    # remote_execution.enable_redis_availability_monitoring (bool): If \n    # enabled, the execution server will detect if Redis has lost state and \n    # will ask Bazel to retry executions.\n    enable_redis_availability_monitoring: false\n    # remote_execution.enable_remote_exec (bool): If true, enable remote-exec. \n    # ** Enterprise only **\n    enable_remote_exec: true\n    # remote_execution.enable_user_owned_executors (bool): If enabled, users \n    # can register their own executors with the scheduler.\n    enable_user_owned_executors: false\n    # remote_execution.enable_workflows (bool): Whether to enable BuildBuddy \n    # workflows.\n    enable_workflows: false\n    # remote_execution.force_user_owned_darwin_executors (bool): If enabled, \n    # darwin actions will always run on user-owned executors.\n    force_user_owned_darwin_executors: false\n    # remote_execution.redis_pubsub_pool_size (int): Maximum number of \n    # connections used for waiting for execution updates.\n    redis_pubsub_pool_size: 10000\n    # remote_execution.redis_target (string): A Redis target for storing \n    # remote execution state. Falls back to app.default_redis_target if \n    # unspecified. Required for remote execution. To ease migration, the redis \n    # target from the cache config will be used if neither this value nor \n    # app.default_redis_target are specified.\n    redis_target: ""\n    # remote_execution.remove_stale_executors (bool): If true, executors are \n    # removed if they are not heard from for a prolonged amount of time.\n    remove_stale_executors: false\n    # remote_execution.require_executor_authorization (bool): If true, \n    # executors connecting to this server must provide a valid executor API \n    # key.\n    require_executor_authorization: false\n    sharded_redis:\n        # remote_execution.sharded_redis.password (string): Redis password\n        password: ""\n        # remote_execution.sharded_redis.shards ([]string): Ordered list of \n        # Redis shard addresses.\n        shards: []\n        # remote_execution.sharded_redis.username (string): Redis username\n        username: ""\n    # remote_execution.shared_executor_pool_group_id (string): Group ID that \n    # owns the shared executor pool.\n    shared_executor_pool_group_id: ""\n    task_size_model:\n        # remote_execution.task_size_model.enabled (bool): Whether to enable \n        # model-based task size prediction.\n        enabled: false\n        # remote_execution.task_size_model.features_config_path (string): Path \n        # pointing to features.json config file.\n        features_config_path: ""\n        # remote_execution.task_size_model.serving_address (string): gRPC \n        # address pointing to TensorFlow Serving prediction service with task \n        # size models (cpu, mem).\n        serving_address: ""\n    # remote_execution.use_measured_task_sizes (bool): Whether to use measured \n    # usage stats to determine task sizes.\n    use_measured_task_sizes: false\n    # remote_execution.workflows_ci_runner_bazel_command (string): Bazel \n    # command to be used by the CI runner.\n    workflows_ci_runner_bazel_command: ""\n    # remote_execution.workflows_ci_runner_debug (bool): Whether to run the CI \n    # runner in debug mode.\n    workflows_ci_runner_debug: false\n    # remote_execution.workflows_default_image (string): The default \n    # container-image property to use for workflows. Must include docker:// \n    # prefix if applicable.\n    workflows_default_image: docker://gcr.io/flame-public/buildbuddy-ci-runner@sha256:ba33bd1b3acdfe980b958baf7d05c2041c9d4183d15fdf665dd236289d777709\n    # remote_execution.workflows_enable_firecracker (bool): Whether to enable \n    # firecracker for Linux workflow actions.\n    workflows_enable_firecracker: false\n    # remote_execution.workflows_linux_compute_units (int): Number of \n    # BuildBuddy compute units (BCU) to reserve for Linux workflow actions.\n    workflows_linux_compute_units: 3\n    # remote_execution.workflows_mac_compute_units (int): Number of BuildBuddy \n    # compute units (BCU) to reserve for Mac workflow actions.\n    workflows_mac_compute_units: 3\n    # remote_execution.workflows_pool_name (string): The executor pool to use \n    # for workflow actions. Defaults to the default executor pool if not \n    # specified.\n    workflows_pool_name: ""\nssl:\n    # ssl.cert_file (string): Path to a PEM encoded certificate file to use \n    # for TLS if not using ACME.\n    cert_file: ""\n    # ssl.client_ca_cert_file (string): Path to a PEM encoded certificate \n    # authority file used to issue client certificates for mTLS auth.\n    client_ca_cert_file: ""\n    # ssl.client_ca_key_file (string): Path to a PEM encoded certificate \n    # authority key file used to issue client certificates for mTLS auth.\n    client_ca_key_file: ""\n    # ssl.client_cert_lifespan (time.Duration): The duration client \n    # certificates are valid for. Ex: \'730h\' for one month. If not set, \n    # defaults to 100 years.\n    client_cert_lifespan: 876000h0m0s\n    # ssl.default_host (string): Host name to use for ACME generated cert if \n    # TLS request does not contain SNI.\n    default_host: ""\n    # ssl.enable_ssl (bool): Whether or not to enable SSL/TLS on gRPC \n    # connections (gRPCS).\n    enable_ssl: false\n    # ssl.host_whitelist ([]string): Cloud-Only\n    host_whitelist: []\n    # ssl.key_file (string): Path to a PEM encoded key file to use for TLS if \n    # not using ACME.\n    key_file: ""\n    # ssl.self_signed (bool): If true, a self-signed cert will be generated \n    # for TLS termination.\n    self_signed: false\n    # ssl.upgrade_insecure (bool): True if http requests should be redirected \n    # to https\n    upgrade_insecure: false\n    # ssl.use_acme (bool): Whether or not to automatically configure SSL certs \n    # using ACME. If ACME is enabled, cert_file and key_file should not be \n    # set.\n    use_acme: false\nstorage:\n    aws_s3:\n        # storage.aws_s3.bucket (string): The AWS S3 bucket to store files in.\n        bucket: ""\n        # storage.aws_s3.credentials_profile (string): A custom credentials \n        # profile to use.\n        credentials_profile: ""\n        # storage.aws_s3.disable_ssl (bool): Disables the use of SSL, useful \n        # for configuring the use of MinIO.\n        disable_ssl: false\n        # storage.aws_s3.endpoint (string): The AWS endpoint to use, useful \n        # for configuring the use of MinIO.\n        endpoint: ""\n        # storage.aws_s3.region (string): The AWS region.\n        region: ""\n        # storage.aws_s3.role_arn (string): The role ARN to use for web \n        # identity auth.\n        role_arn: ""\n        # storage.aws_s3.role_session_name (string): The role session name to \n        # use for web identity auth.\n        role_session_name: ""\n        # storage.aws_s3.s3_force_path_style (bool): Force path style urls for \n        # objects, useful for configuring the use of MinIO.\n        s3_force_path_style: false\n        # storage.aws_s3.static_credentials_id (string): Static credentials ID \n        # to use, useful for configuring the use of MinIO.\n        static_credentials_id: ""\n        # storage.aws_s3.static_credentials_secret (string): Static \n        # credentials secret to use, useful for configuring the use of MinIO.\n        static_credentials_secret: ""\n        # storage.aws_s3.static_credentials_token (string): Static credentials \n        # token to use, useful for configuring the use of MinIO.\n        static_credentials_token: ""\n        # storage.aws_s3.web_identity_token_file (string): The file path to \n        # the web identity token file.\n        web_identity_token_file: ""\n    azure:\n        # storage.azure.account_key (string): The key for the Azure storage \n        # account\n        account_key: ""\n        # storage.azure.account_name (string): The name of the Azure storage \n        # account\n        account_name: ""\n        # storage.azure.container_name (string): The name of the Azure storage \n        # container\n        container_name: ""\n    # storage.chunk_file_size_bytes (int): How many bytes to buffer in memory \n    # before flushing a chunk of build protocol data to disk.\n    chunk_file_size_bytes: 3000000\n    disk:\n        # storage.disk.root_directory (string): The root directory to store \n        # all blobs in, if using disk based storage.\n        root_directory: /tmp/buildbuddy\n        # storage.disk.use_v2_layout (bool): If enabled, files will be stored \n        # using the v2 layout. See disk_cache.MigrateToV2Layout for a \n        # description.\n        use_v2_layout: false\n    # storage.enable_chunked_event_logs (bool): If true, Event logs will be \n    # stored separately from the invocation proto in chunks.\n    enable_chunked_event_logs: false\n    gcs:\n        # storage.gcs.bucket (string): The name of the GCS bucket to store \n        # build artifact files in.\n        bucket: ""\n        # storage.gcs.credentials_file (string): A path to a JSON credentials \n        # file that will be used to authenticate to GCS.\n        credentials_file: ""\n        # storage.gcs.project_id (string): The Google Cloud project ID of the \n        # project owning the above credentials and GCS bucket.\n        project_id: ""\n    # storage.tempdir (string): Root directory for temporary files. Defaults \n    # to the OS-specific temp dir.\n    tempdir: /tmp\n    # storage.ttl_seconds (int): The time, in seconds, to keep invocations \n    # before deletion. 0 disables invocation deletion.\n    ttl_seconds: 0\n')))}_.isMDXComponent=!0;var u=["components"],p={toc:[]};function h(e){var t=e.components,n=(0,a.Z)(e,u);return(0,i.kt)("wrapper",(0,o.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'# Unstructured settings\n\n# debug_stream_command_outputs (bool): If true, stream command outputs to the \n# terminal. Intended for debugging purposes only and should not be used in \n# production.\ndebug_stream_command_outputs: false\n# debug_use_local_images_only (bool): Do not pull OCI images and only used \n# locally cached images. This can be set to test local image builds during \n# development without needing to push to a container registry. Not intended \n# for production use.\ndebug_use_local_images_only: false\n# docker_cap_add (string): Sets --cap-add= on the docker command. Comma \n# separated.\ndocker_cap_add: ""\n# drop_invocation_pk_cols (bool): If true, attempt to drop invocation PK cols\ndrop_invocation_pk_cols: false\n# grpc_port (int): The port to listen for gRPC traffic on\ngrpc_port: 1985\n# grpcs_port (int): The port to listen for gRPCS traffic on\ngrpcs_port: 1986\n# internal_grpc_port (int): The port to listen for internal gRPC traffic on\ninternal_grpc_port: 1987\n# internal_grpcs_port (int): The port to listen for internal gRPCS traffic on\ninternal_grpcs_port: 1988\n# listen (string): The interface to listen on (default: 0.0.0.0)\nlisten: 0.0.0.0\n# max_shutdown_duration (time.Duration): Time to wait for shutdown\nmax_shutdown_duration: 25s\n# monitoring_port (int): The port to listen for monitoring traffic on\nmonitoring_port: 9090\n# podman_runtime (string): Enables running podman with other runtimes, like \n# gVisor (runsc).\npodman_runtime: ""\n# port (int): The port to listen for HTTP traffic on\nport: 8080\n# redis_command_buffer_flush_period (time.Duration): How long to wait between \n# flushing buffered redis commands. Setting this to 0 will disable buffering \n# at the cost of higher redis QPS.\nredis_command_buffer_flush_period: 250ms\n# server_type (string): The server type to match on health checks\nserver_type: prod-buildbuddy-executor\n# shutdown_lameduck_duration (time.Duration): If set, the server will be \n# marked unready but not run shutdown functions until this period passes.\nshutdown_lameduck_duration: 0s\n# zone_override (string): A value that will override the auto-detected zone. \n# Ignored if empty\nzone_override: ""\n\n# Structured settings\n\napp:\n    # app.admin_only_create_group (bool): If true, only admins of an existing \n    # group can create a new groups.\n    admin_only_create_group: false\n    # app.build_buddy_url (URL): The external URL where your BuildBuddy \n    # instance can be found.\n    build_buddy_url: http://localhost:8080\n    # app.cache_api_url (URL): Overrides the default remote cache protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    cache_api_url: ""\n    # app.default_redis_target (string): A Redis target for storing remote \n    # shared state. To ease migration, the redis target from the remote \n    # execution config will be used if this value is not specified.\n    default_redis_target: ""\n    default_sharded_redis:\n        # app.default_sharded_redis.password (string): Redis password\n        password: ""\n        # app.default_sharded_redis.shards ([]string): Ordered list of Redis \n        # shard addresses.\n        shards: []\n        # app.default_sharded_redis.username (string): Redis username\n        username: ""\n    # app.enable_grpc_metrics_by_group_id (bool): If enabled, grpc metrics by \n    # group ID will be recorded\n    enable_grpc_metrics_by_group_id: false\n    # app.enable_prometheus_histograms (bool): If true, collect prometheus \n    # histograms for all RPCs\n    enable_prometheus_histograms: true\n    # app.enable_structured_logging (bool): If true, log messages will be \n    # json-formatted.\n    enable_structured_logging: false\n    # app.events_api_url (URL): Overrides the default build event protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    events_api_url: ""\n    # app.grpc_max_recv_msg_size_bytes (int): Configures the max GRPC receive \n    # message size [bytes]\n    grpc_max_recv_msg_size_bytes: 50000000\n    # app.grpc_over_http_port_enabled (bool): Cloud-Only\n    grpc_over_http_port_enabled: false\n    # app.ignore_forced_tracing_header (bool): If set, we will not honor the \n    # forced tracing header.\n    ignore_forced_tracing_header: false\n    # app.log_enable_gcp_logging_format (bool): If true, the output structured \n    # logs will be compatible with format expected by GCP Logging.\n    log_enable_gcp_logging_format: false\n    # app.log_error_stack_traces (bool): If true, stack traces will be printed \n    # for errors that have them.\n    log_error_stack_traces: false\n    # app.log_gcp_log_id (string): The log ID to log to in GCP (if any).\n    log_gcp_log_id: ""\n    # app.log_gcp_project_id (string): The project ID to log to in GCP (if \n    # any).\n    log_gcp_project_id: ""\n    # app.log_include_short_file_name (bool): If true, log messages will \n    # include shortened originating file name.\n    log_include_short_file_name: false\n    # app.log_level (string): The desired log level. Logs with a level >= this \n    # level will be emitted. One of {\'fatal\', \'error\', \'warn\', \'info\', \n    # \'debug\'}\n    log_level: info\n    # app.trace_fraction (float64): Fraction of requests to sample for \n    # tracing.\n    trace_fraction: 0\n    # app.trace_fraction_overrides ([]string): Tracing fraction override based \n    # on name in format name=fraction.\n    trace_fraction_overrides: []\n    # app.trace_jaeger_collector (string): Address of the Jager collector \n    # endpoint where traces will be sent.\n    trace_jaeger_collector: ""\n    # app.trace_project_id (string): Optional GCP project ID to export traces \n    # to. If not specified, determined from default credentials or metadata \n    # server if running on GCP.\n    trace_project_id: ""\n    # app.trace_service_name (string): Name of the service to associate with \n    # traces.\n    trace_service_name: ""\nauth:\n    # auth.admin_group_id (string): ID of a group whose members can perform \n    # actions only accessible to server admins.\n    admin_group_id: ""\n    # auth.api_key_group_cache_ttl (time.Duration): TTL for API Key to Group \n    # caching. Set to \'0\' to disable cache.\n    api_key_group_cache_ttl: 5m0s\n    # auth.disable_refresh_token (bool): If true, the offline_access scope \n    # which requests refresh tokens will not be requested.\n    disable_refresh_token: false\n    # auth.enable_anonymous_usage (bool): If true, unauthenticated build \n    # uploads will still be allowed but won\'t be associated with your \n    # organization.\n    enable_anonymous_usage: false\n    # auth.enable_self_auth (bool): If true, enables a single user login via \n    # an oauth provider on the buildbuddy server. Recommend use only when \n    # server is behind a firewall; this option may allow anyone with access to \n    # the webpage admin rights to your buildbuddy installation. ** Enterprise \n    # only **\n    enable_self_auth: false\n    # auth.force_approval (bool): If true, when a user doesn\'t have a session \n    # (first time logging in, or manually logged out) force the auth provider \n    # to show the consent screen allowing the user to select an account if \n    # they have multiple. This isn\'t supported by all auth providers.\n    force_approval: false\n    # auth.https_only_cookies (bool): If true, cookies will only be set over \n    # https connections.\n    https_only_cookies: false\n    # auth.jwt_claims_cache_ttl (time.Duration): TTL for JWT string to parsed \n    # claims caching. Set to \'0\' to disable cache.\n    jwt_claims_cache_ttl: 15s\n    # auth.jwt_key (string): The key to use when signing JWT tokens.\n    jwt_key: set_the_jwt_in_config\n    # auth.oauth_providers ([]auth.OauthProvider): The list of oauth providers \n    # to use to authenticate.\n    oauth_providers: []\n    # For example:\n    # - issuer_url: "" # The issuer URL of this OIDC Provider. (type: string)\n    #   client_id: "" # The oauth client ID. (type: string)\n    #   client_secret: "" # The oauth client secret. (type: string)\n    #   slug: "" # The slug of this OIDC Provider. (type: string)\ncache:\n    client:\n        # cache.client.enable_download_compression (bool): If true, enable \n        # compression of downloads from remote caches\n        enable_download_compression: false\n        # cache.client.enable_upload_compression (bool): If true, enable \n        # compression of uploads to remote caches\n        enable_upload_compression: false\n    # cache.detailed_stats_enabled (bool): Whether to enable detailed stats \n    # recording for all cache requests.\n    detailed_stats_enabled: false\n    # cache.enable_tree_caching (bool): If true, cache GetTree responses (full \n    # and partial)\n    enable_tree_caching: true\n    gcs:\n        # cache.gcs.bucket (string): The name of the GCS bucket to store cache \n        # files in.\n        bucket: ""\n        # cache.gcs.credentials_file (string): A path to a JSON credentials \n        # file that will be used to authenticate to GCS.\n        credentials_file: ""\n        # cache.gcs.project_id (string): The Google Cloud project ID of the \n        # project owning the above credentials and GCS bucket.\n        project_id: ""\n        # cache.gcs.ttl_days (int64): The period after which cache files \n        # should be TTLd. Disabled if 0.\n        ttl_days: 0\n    # cache.memcache_targets ([]string): Deprecated. Use Redis Target instead.\n    memcache_targets: []\n    redis:\n        # cache.redis.max_value_size_bytes (int64): The maximum value size to \n        # cache in redis (in bytes).\n        max_value_size_bytes: 10000000\n        # cache.redis.redis_target (string): A redis target for improved \n        # Caching/RBE performance. Target can be provided as either a redis \n        # connection URI or a host:port pair. URI schemas supported: \n        # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n        # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise \n        # only **\n        redis_target: ""\n        sharded:\n            # cache.redis.sharded.password (string): Redis password\n            password: ""\n            # cache.redis.sharded.shards ([]string): Ordered list of Redis \n            # shard addresses.\n            shards: []\n            # cache.redis.sharded.username (string): Redis username\n            username: ""\n    # cache.redis_target (string): A redis target for improved Caching/RBE \n    # performance. Target can be provided as either a redis connection URI or \n    # a host:port pair. URI schemas supported: \n    # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n    # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise only \n    # **\n    redis_target: ""\n    s3:\n        # cache.s3.bucket (string): The AWS S3 bucket to store files in.\n        bucket: ""\n        # cache.s3.credentials_profile (string): A custom credentials profile \n        # to use.\n        credentials_profile: ""\n        # cache.s3.disable_ssl (bool): Disables the use of SSL, useful for \n        # configuring the use of MinIO.\n        disable_ssl: false\n        # cache.s3.endpoint (string): The AWS endpoint to use, useful for \n        # configuring the use of MinIO.\n        endpoint: ""\n        # cache.s3.region (string): The AWS region.\n        region: ""\n        # cache.s3.role_arn (string): The role ARN to use for web identity \n        # auth.\n        role_arn: ""\n        # cache.s3.role_session_name (string): The role session name to use \n        # for web identity auth.\n        role_session_name: ""\n        # cache.s3.s3_force_path_style (bool): Force path style urls for \n        # objects, useful for configuring the use of MinIO.\n        s3_force_path_style: false\n        # cache.s3.static_credentials_id (string): Static credentials ID to \n        # use, useful for configuring the use of MinIO.\n        static_credentials_id: ""\n        # cache.s3.static_credentials_secret (string): Static credentials \n        # secret to use, useful for configuring the use of MinIO.\n        static_credentials_secret: ""\n        # cache.s3.static_credentials_token (string): Static credentials token \n        # to use, useful for configuring the use of MinIO.\n        static_credentials_token: ""\n        # cache.s3.ttl_days (int64): The period after which cache files should \n        # be TTLd. Disabled if 0.\n        ttl_days: 0\n        # cache.s3.web_identity_token_file (string): The file path to the web \n        # identity token file.\n        web_identity_token_file: ""\n    # cache.tree_cache_seed (string): If set, hash this with digests before \n    # caching / reading from tree cache\n    tree_cache_seed: treecache-07012022\n    # cache.zstd_transcoding_enabled (bool): Whether to accept requests to \n    # read/write zstd-compressed blobs, compressing/decompressing \n    # outgoing/incoming blobs on the fly.\n    zstd_transcoding_enabled: true\nexecutor:\n    # executor.api_key (string): API Key used to authorize the executor with \n    # the BuildBuddy app server.\n    api_key: ""\n    # executor.app_target (string): The GRPC url of a buildbuddy app server.\n    app_target: grpcs://remote.buildbuddy.io\n    bare:\n        # executor.bare.enable_stats (bool): Whether to enable stats for bare \n        # command execution.\n        enable_stats: false\n    # executor.container_registries ([]container.ContainerRegistry)\n    container_registries: []\n    # For example:\n    # - hostnames: [] # (type: []string)\n    #   username: "" # (type: string)\n    #   password: "" # (type: string)\n\n    # executor.context_based_shutdown_enabled (bool): Whether to remove \n    # runners using context cancelation. This is a transitional flag that will \n    # be removed in a future executor version.\n    context_based_shutdown_enabled: false\n    # executor.default_image (string): The default docker image to use to warm \n    # up executors or if no platform property is set. Ex: \n    # gcr.io/flame-public/executor-docker-default:enterprise-v1.5.4\n    default_image: gcr.io/flame-public/executor-docker-default:enterprise-v1.6.0\n    # executor.default_isolation_type (string): The default workload isolation \n    # type when no type is specified in an action. If not set, we use the \n    # first of the following that is set: docker, firecracker, podman, or \n    # barerunner\n    default_isolation_type: ""\n    # executor.default_xcode_version (string): Sets the default Xcode version \n    # number to use if an action doesn\'t specify one. If not set, \n    # /Applications/Xcode.app/ is used.\n    default_xcode_version: ""\n    # executor.die_on_firecracker_failure (bool): Makes the host executor \n    # process die if any command orchestrating or running Firecracker fails. \n    # Useful for capturing failures preemptively. WARNING: using this option \n    # MAY leave the host machine in an unhealthy state on Firecracker failure; \n    # some post-hoc cleanup may be necessary.\n    die_on_firecracker_failure: false\n    # executor.disable_local_cache (bool): If true, a local file cache will \n    # not be used.\n    disable_local_cache: false\n    # executor.docker_devices ([]container.DockerDeviceMapping): Configure \n    # (docker) devices that will be available inside the sandbox container. \n    # Format is \n    # --executor.docker_devices=\'[{"PathOnHost":"/dev/foo","PathInContainer":"/some/dest","CgroupPermissions":"see,docker,docs"}]\'\n    docker_devices: []\n    # For example:\n    # - path_on_host: "" # path to device that should be mapped from the host. (type: string)\n    #   path_in_container: "" # path under which the device will be present in container. (type: string)\n    #   cgroup_permissions: "" # cgroup permissions that should be assigned to device. (type: string)\n\n    # executor.docker_inherit_user_ids (bool): If set, run docker containers \n    # using the same uid and gid as the user running the executor process.\n    docker_inherit_user_ids: false\n    # executor.docker_mount_mode (string): Sets the mount mode of volumes \n    # mounted to docker images. Useful if running on SELinux \n    # https://www.projectatomic.io/blog/2015/06/using-volumes-with-docker-can-cause-problems-with-selinux/\n    docker_mount_mode: ""\n    # executor.docker_net_host (bool): Sets --net=host on the docker command. \n    # Intended for local development only. **DEPRECATED** Use \n    # --executor.docker_network=host instead.\n    docker_net_host: false\n    # executor.docker_network (string): If set, set docker/podman --network to \n    # this value by default. Can be overridden per-action with the \n    # `dockerNetwork` exec property, which accepts values \'off\' \n    # (--network=none) or \'bridge\' (--network=<default>).\n    docker_network: ""\n    # executor.docker_sibling_containers (bool): If set, mount the configured \n    # Docker socket to containers spawned for each action, to enable \n    # Docker-out-of-Docker (DooD). Takes effect only if docker_socket is also \n    # set. Should not be set by executors that can run untrusted code.\n    docker_sibling_containers: false\n    # executor.docker_socket (string): If set, run execution commands in \n    # docker using the provided socket.\n    docker_socket: ""\n    # executor.docker_volumes ([]string): Additional --volume arguments to be \n    # passed to docker or podman.\n    docker_volumes: []\n    # executor.enable_bare_runner (bool): Enables running execution commands \n    # directly on the host without isolation.\n    enable_bare_runner: false\n    # executor.enable_firecracker (bool): Enables running execution commands \n    # inside of firecracker VMs\n    enable_firecracker: false\n    # executor.enable_podman (bool): Enables running execution commands inside \n    # podman container.\n    enable_podman: false\n    # executor.enable_sandbox (bool): Enables running execution commands \n    # inside of sandbox-exec.\n    enable_sandbox: false\n    # executor.enable_vfs (bool): Whether FUSE based filesystem is enabled.\n    enable_vfs: false\n    # executor.exclusive_task_scheduling (bool): If true, only one task will \n    # be scheduled at a time. Default is false\n    exclusive_task_scheduling: false\n    # executor.extra_env_vars ([]string): Additional environment variables to \n    # pass to remotely executed actions. i.e. MY_ENV_VAR=foo\n    extra_env_vars: []\n    # executor.firecracker_cgroup_version (string): Specifies the cgroup \n    # version for firecracker to use.\n    firecracker_cgroup_version: ""\n    # executor.firecracker_mount_workspace_file (bool): Enables mounting \n    # workspace filesystem to improve performance of copying action outputs.\n    firecracker_mount_workspace_file: false\n    # executor.host_root_directory (string): Path on the host where the \n    # executor container root directory is mounted.\n    host_root_directory: ""\n    # executor.local_cache_directory (string): A local on-disk cache \n    # directory. Must be on the same device (disk partition, Docker volume, \n    # etc.) as the configured root_directory, since files are hard-linked to \n    # this cache for performance reasons. Otherwise, \'Invalid cross-device \n    # link\' errors may result.\n    local_cache_directory: /tmp/buildbuddy/filecache\n    # executor.local_cache_size_bytes (int64): The maximum size, in bytes, to \n    # use for the local on-disk cache\n    local_cache_size_bytes: 1000000000\n    # executor.memory_bytes (int64): Optional maximum memory to allocate to \n    # execution tasks (approximate). Cannot set both this option and the \n    # SYS_MEMORY_BYTES env var.\n    memory_bytes: 0\n    # executor.millicpu (int64): Optional maximum CPU milliseconds to allocate \n    # to execution tasks (approximate). Cannot set both this option and the \n    # SYS_MILLICPU env var.\n    millicpu: 0\n    podman:\n        # executor.podman.cpu_usage_path_template (string): Go template \n        # specifying a path pointing to a container\'s total CPU usage, in CPU \n        # nanoseconds. Templated with `ContainerID`.\n        cpu_usage_path_template: /sys/fs/cgroup/cpuacct/libpod_parent/libpod-{{.ContainerID}}/cpuacct.usage\n        # executor.podman.enable_stats (bool): Whether to enable cgroup-based \n        # podman stats.\n        enable_stats: false\n        image_streaming:\n            # executor.podman.image_streaming.enabled (bool): Whether \n            # container image streaming is enabled by default\n            enabled: false\n            # executor.podman.image_streaming.registry_grpc_target (string): \n            # gRPC endpoint of BuildBuddy registry\n            registry_grpc_target: ""\n            # executor.podman.image_streaming.registry_http_target (string): \n            # HTTP endpoint of the BuildBuddy registry\n            registry_http_target: ""\n        # executor.podman.memory_usage_path_template (string): Go template \n        # specifying a path pointing to a container\'s current memory usage, in \n        # bytes. Templated with `ContainerID`.\n        memory_usage_path_template: /sys/fs/cgroup/memory/libpod_parent/libpod-{{.ContainerID}}/memory.usage_in_bytes\n        # executor.podman.pull_timeout (time.Duration): Timeout for image \n        # pulls.\n        pull_timeout: 10m0s\n    # executor.pool (string): Executor pool name. Only one of this config \n    # option or the MY_POOL environment variable should be specified.\n    pool: ""\n    # executor.preserve_existing_netns (bool): Preserve existing bb-executor \n    # net namespaces. By default all "bb-executor" net namespaces are removed \n    # on executor startup, but if multiple executors are running on the same \n    # machine this behavior should be disabled to prevent them interfering \n    # with each other.\n    preserve_existing_netns: false\n    # executor.root_directory (string): The root directory to use for build \n    # files.\n    root_directory: /tmp/buildbuddy/remote_build\n    # executor.route_prefix (string): The prefix in the ip route to locate a \n    # device: either \'default\' or the ip range of the subnet e.g. \n    # 172.24.0.0/18\n    route_prefix: default\n    runner_pool:\n        # executor.runner_pool.max_runner_count (int): Maximum number of \n        # recycled RBE runners that can be pooled at once. Defaults to a value \n        # derived from estimated CPU usage, max RAM, allocated CPU, and \n        # allocated memory.\n        max_runner_count: 0\n        # executor.runner_pool.max_runner_disk_size_bytes (int64): Maximum \n        # disk size for a recycled runner; runners exceeding this threshold \n        # are not recycled. Defaults to 16GB.\n        max_runner_disk_size_bytes: 16000000000\n        # executor.runner_pool.max_runner_memory_usage_bytes (int64): Maximum \n        # memory usage for a recycled runner; runners exceeding this threshold \n        # are not recycled. Defaults to 1/10 of total RAM allocated to the \n        # executor. (Only supported for Docker-based executors).\n        max_runner_memory_usage_bytes: 8000000000\n    # executor.shutdown_cleanup_duration (time.Duration): The minimum duration \n    # during the shutdown window to allocate for cleaning up containers. This \n    # is capped to the value of `max_shutdown_duration`.\n    shutdown_cleanup_duration: 15s\n    # executor.startup_warmup_max_wait_secs (int64): Maximum time to block \n    # startup while waiting for default image to be pulled. Default is no \n    # wait.\n    startup_warmup_max_wait_secs: 0\n    # executor.warmup_timeout_secs (int64): The default time (in seconds) to \n    # wait for an executor to warm up i.e. download the default docker image. \n    # Default is 120s\n    warmup_timeout_secs: 120\n    # executor.warmup_workflow_images (bool): Whether to warm up the Linux \n    # workflow images (firecracker only).\n    warmup_workflow_images: false\nmonitoring:\n    basic_auth:\n        # monitoring.basic_auth.password (string): Optional password for basic \n        # auth on the monitoring port.\n        password: ""\n        # monitoring.basic_auth.username (string): Optional username for basic \n        # auth on the monitoring port.\n        username: ""\n    # monitoring.ssl_port (int): If non-negative, the SSL port to listen for \n    # monitoring traffic on. `ssl` config must have `ssl_enabled: true` and be \n    # properly configured.\n    ssl_port: -1\nolap_database:\n    # olap_database.cluster_name (string): The cluster name of the database\n    cluster_name: \'{cluster}\'\n    # olap_database.enable_data_replication (bool): If true, data replication \n    # is enabled.\n    enable_data_replication: false\n    # olap_database.replica_name (string): The replica name of the table in \n    # zookeeper\n    replica_name: \'{replica}\'\n    # olap_database.zoo_path (string): The path to the table name in \n    # zookeeper, used to set up data replication\n    zoo_path: /clickhouse/{installation}/{cluster}/tables/{shard}/{database}/{table}\nremote_execution:\n    # remote_execution.enable_remote_exec (bool): If true, enable remote-exec. \n    # ** Enterprise only **\n    enable_remote_exec: true\n    # remote_execution.redis_target (string): A Redis target for storing \n    # remote execution state. Falls back to app.default_redis_target if \n    # unspecified. Required for remote execution. To ease migration, the redis \n    # target from the cache config will be used if neither this value nor \n    # app.default_redis_target are specified.\n    redis_target: ""\n    sharded_redis:\n        # remote_execution.sharded_redis.password (string): Redis password\n        password: ""\n        # remote_execution.sharded_redis.shards ([]string): Ordered list of \n        # Redis shard addresses.\n        shards: []\n        # remote_execution.sharded_redis.username (string): Redis username\n        username: ""\n    task_size_model:\n        # remote_execution.task_size_model.enabled (bool): Whether to enable \n        # model-based task size prediction.\n        enabled: false\n        # remote_execution.task_size_model.features_config_path (string): Path \n        # pointing to features.json config file.\n        features_config_path: ""\n        # remote_execution.task_size_model.serving_address (string): gRPC \n        # address pointing to TensorFlow Serving prediction service with task \n        # size models (cpu, mem).\n        serving_address: ""\n    # remote_execution.use_measured_task_sizes (bool): Whether to use measured \n    # usage stats to determine task sizes.\n    use_measured_task_sizes: false\nssl:\n    # ssl.cert_file (string): Path to a PEM encoded certificate file to use \n    # for TLS if not using ACME.\n    cert_file: ""\n    # ssl.client_ca_cert_file (string): Path to a PEM encoded certificate \n    # authority file used to issue client certificates for mTLS auth.\n    client_ca_cert_file: ""\n    # ssl.client_ca_key_file (string): Path to a PEM encoded certificate \n    # authority key file used to issue client certificates for mTLS auth.\n    client_ca_key_file: ""\n    # ssl.client_cert_lifespan (time.Duration): The duration client \n    # certificates are valid for. Ex: \'730h\' for one month. If not set, \n    # defaults to 100 years.\n    client_cert_lifespan: 876000h0m0s\n    # ssl.default_host (string): Host name to use for ACME generated cert if \n    # TLS request does not contain SNI.\n    default_host: ""\n    # ssl.enable_ssl (bool): Whether or not to enable SSL/TLS on gRPC \n    # connections (gRPCS).\n    enable_ssl: false\n    # ssl.host_whitelist ([]string): Cloud-Only\n    host_whitelist: []\n    # ssl.key_file (string): Path to a PEM encoded key file to use for TLS if \n    # not using ACME.\n    key_file: ""\n    # ssl.self_signed (bool): If true, a self-signed cert will be generated \n    # for TLS termination.\n    self_signed: false\n    # ssl.upgrade_insecure (bool): True if http requests should be redirected \n    # to https\n    upgrade_insecure: false\n    # ssl.use_acme (bool): Whether or not to automatically configure SSL certs \n    # using ACME. If ACME is enabled, cert_file and key_file should not be \n    # set.\n    use_acme: false\n')))}h.isMDXComponent=!0;var f=["components"],g={id:"config-all-options",title:"All Options",sidebar_label:"All Options"},b=void 0,m={unversionedId:"config-all-options",id:"config-all-options",title:"All Options",description:"Provided below are working, documented YAML configs for each BuildBuddy binary",source:"@site/../docs/config-all-options.mdx",sourceDirName:".",slug:"/config-all-options",permalink:"/docs/config-all-options",editUrl:"https://github.com/buildbuddy-io/buildbuddy/edit/master/docs/../docs/config-all-options.mdx",tags:[],version:"current",lastUpdatedBy:"Son Luong Ngoc",lastUpdatedAt:1676305645,formattedLastUpdatedAt:"2/13/2023",frontMatter:{id:"config-all-options",title:"All Options",sidebar_label:"All Options"},sidebar:"someSidebar",previous:{title:"Flags",permalink:"/docs/config-flags"}},y=[{value:"BuildBuddy Server (FOSS)",id:"buildbuddy-server-foss",children:[],level:3},{value:"BuildBuddy Server (Enterprise)",id:"buildbuddy-server-enterprise",children:[],level:3},{value:"BuildBuddy Executor",id:"buildbuddy-executor",children:[],level:3}],w={toc:y};function k(e){var t=e.components,n=(0,a.Z)(e,f);return(0,i.kt)("wrapper",(0,o.Z)({},w,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Provided below are working, documented YAML configs for each BuildBuddy binary\ncontaining every option that that binary accepts, each set to the default value\nfor that option. Any option that can be specified in the YAML config can also be\npassed on the command line. For nested options, be sure to write out the full\nYAML path, with a ",(0,i.kt)("inlineCode",{parentName:"p"},".")," separating each part."),(0,i.kt)("p",null,"For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"storage:\n    disk:\n        root_directory: /tmp/buildbuddy\n")),(0,i.kt)("p",null,"becomes:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},'buildbuddy -storage.disk.root_directory="/tmp/buildbuddy"\n')),(0,i.kt)("p",null,"For specifying lists of structures using flags on the command line, use the JSON\nrepresentation of the list you wish to concatenate to the end or the element you\nwish to append:"),(0,i.kt)("p",null,"For example, given the following schema:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'cache:\n    disk:\n        partitions: [] # type: []disk.Partition\n        # e.g.:\n        # - id: "" # type: string\n        #   max_size_bytes: 0 # type: int\n')),(0,i.kt)("p",null,"We see that ",(0,i.kt)("inlineCode",{parentName:"p"},"cache.disk.partitions")," is configured as a list of ",(0,i.kt)("inlineCode",{parentName:"p"},"disk.Partition"),". In YAML, we'd normally configure it like this:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'cache:\n    disk:\n        partitions:\n        - id: "1GB"\n          max_size_bytes: 1073741824\n        - id: "2GB"\n          max_size_bytes: 2147483648\n')),(0,i.kt)("p",null,"The flag equivalent of this example would be:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},'buildbuddy -cache.disk.partitions=\'{"id": "1GB", "max_size_bytes": 1073741824}\' -cache.disk.partitions=\'{"id": "2GB", "max_size_bytes": 2147483648}\'\n')),(0,i.kt)("p",null,"or"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},'buildbuddy -cache.disk.partitions=\'[{"id": "1GB", "max_size_bytes": 1073741824}, {"id": "2GB", "max_size_bytes": 2147483648}]\'\n')),(0,i.kt)("h3",{id:"buildbuddy-server-foss"},"BuildBuddy Server (FOSS)"),(0,i.kt)(l,{mdxType:"FreeServerConfig"}),(0,i.kt)("h3",{id:"buildbuddy-server-enterprise"},"BuildBuddy Server (Enterprise)"),(0,i.kt)(_,{mdxType:"EnterpriseServerConfig"}),(0,i.kt)("h3",{id:"buildbuddy-executor"},"BuildBuddy Executor"),(0,i.kt)(h,{mdxType:"ExecutorConfig"}))}k.isMDXComponent=!0}}]);